---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "1.5&nbsp; State space models"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 4.3
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache=TRUE,
  warning=FALSE,
  message=FALSE)
library(fpp2)
options(digits=4, width=55)
```

# Innovations state space models
## Methods V Models

### Exponential smoothing methods

  * Algorithms that return point forecasts.

\pause

### Innovations state space models

  * Generate same point forecasts but can also generate forecast intervals.
  * A stochastic (or random) data generating process that can generate an entire forecast distribution.
  * Allow for "proper" model selection.

## ETS models

   * Each model has an *observation* equation and *state* equations, one for each state (level, trend, seasonal), i.e., state space models.
   * Two models for each method: one with additive and one with multiplicative errors, i.e., in total \alert{18 models}.
   * ETS(Error,Trend,Seasonal):
      * Error $=\{$A,M$\}$
      * Trend $=\{$N,A,A\damped$\}$
      * Seasonal $=\{$N,A,M$\}$.

## Exponential smoothing methods
\fontsize{11.5}{12}\sf

\begin{block}{}\fontsize{11.5}{12}\sf
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & ~(None)~    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.3cm]
N & (None) & N,N & N,A & N,M\\
&&&&\\[-0.3cm]
A & (Additive) & A,N & A,A & A,M\\
&&&&\\[-0.3cm]
A\damped & (Additive damped) & A\damped,N & A\damped,A & A\damped,M
\end{tabular}
\end{block}

\vspace*{10cm}

## Exponential smoothing methods
\fontsize{11.5}{12}\sf

\begin{block}{}\fontsize{11.5}{12}\sf
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & ~(None)~    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.3cm]
N & (None) & N,N & N,A & N,M\\
&&&&\\[-0.3cm]
A & (Additive) & A,N & A,A & A,M\\
&&&&\\[-0.3cm]
A\damped & (Additive damped) & A\damped,N & A\damped,A & A\damped,M
\end{tabular}
\end{block}

\begin{tabular}{l@{}p{2.3cm}@{}c@{}l}
\alert{General n\rlap{otation}}
    &       & ~E T S~  & ~:\hspace*{0.3cm}\textbf{E}xponen\textbf{T}ial \textbf{S}moothing               \\ [-0.1cm]
    & \hfill{$\nearrow$\hspace*{-0.1cm}}        & {$\uparrow$} & {\hspace*{-0.2cm}$\nwarrow$} \\
    & \hfill{\textbf{E}rror\hspace*{0.2cm}} & {\textbf{T}rend}      & {\hspace*{0.2cm}\textbf{S}easonal}
\end{tabular}
\pause\vspace*{-0.4cm}

\alert{Examples:}\newline\footnotesize\vspace*{-0.7cm}

\begin{tabular}{ll}
A,N,N: &Simple exponential smoothing with additive errors\\
A,A,N: &Holt's linear method with additive errors\\

M,A,M: &Multiplicative Holt-Winters' method with multiplicative errors
\end{tabular}

\pause
\alert{\bf There are 18 separate models in the ETS framework}

## A model for SES

\begin{block}{Component form}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&&\pred{y}{t+h}{t} &= \ell_{t}\\
\text{Smoothing equation}&&\ell_{t} &= \alpha y_{t} + (1 - \alpha)\ell_{t-1}
\end{align*}
\end{block}\pause\vspace*{-0.2cm}
Forecast error: $e_t = y_t - \pred{y}{t}{t-1} = y_t - \ell_{t-1}$.\pause
\begin{block}{Error correction form}\vspace*{-0.4cm}
\begin{align*}
y_t &= \ell_{t-1} + e_t\\
\ell_{t}
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t}
\end{align*}
\end{block}\pause\vspace*{-0.3cm}

Assume $e_t = \varepsilon_t\sim\text{NID}(0,\sigma^2)$.

## ETS(A,N,N)

\begin{block}{SES with additive errors}\vspace*{-0.4cm}
\begin{align*}
\text{Observation equation}&& y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State equation}&& \ell_t&=\ell_{t-1}+\alpha \varepsilon_t
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.

  * "innovations" or "single source of error" because same error process, $\varepsilon_t$.
  * Observation equation: relationship between observations and states.
  * State equation(s): evolution of the state(s) through time.

## ETS(A,A,N)

  * Set $\varepsilon_t=y_t-\ell_{t-1}-b_{t-1} \sim \text{NID}(0,\sigma^2)$.
  * Substituting into the error correction equations for Holt's linear method

  \begin{block}{Holt's linear method with additive errors}\vspace*{-0.3cm}
  \begin{align*}
      y_t&=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
      \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
      b_t&=b_{t-1}+\alpha\beta^* \varepsilon_t
  \end{align*}
  \end{block}

  * For simplicity, set $\beta=\alpha \beta^*$.

## ETS(A,A,A)

\begin{block}{Holt-Winters additive method with additive errors}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)}\\
\text{Observation equation}&& y_t&=\ell_{t-1}+b_{t-1}+s_{t-m} + \varepsilon_t\\
\text{State equations}&& \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&        b_t&=b_{t-1}+\beta \varepsilon_t \\
&&s_t &= s_{t-m} + \gamma\varepsilon_t
\end{align*}
\end{block}

* $k$ is integer part of $(h-1)/m$.

## ETS(M,N,N)
\vspace*{-0.2cm}
SES with multiplicative errors:\vspace*{-0.2cm}

  * Specify relative errors  $\varepsilon_t=\frac{y_t-\pred{y}{t}{t-1}}{\pred{y}{t}{t-1}}\sim \text{NID}(0,\sigma^2)$

\pause
\begin{block}{SES with multiplicative errors}\vspace*{-0.3cm}
\begin{align*}
\text{Observation equation}&& y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State equation}&& \ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t)
\end{align*}
\end{block}
\pause\vspace*{-0.3cm}

  * Models with additive and multiplicative errors with the same parameters generate the same point forecasts but different prediction intervals.

## ETS(M,A,N)

\begin{block}{Holt's linear method with multiplicative errors}\vspace*{-0.3cm}
  \begin{align*}
      y_t&=(\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
      \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
      b_t&=b_{t-1}+\beta(\ell_{t-1}+b_{t-1}) \varepsilon_t
  \end{align*}
\end{block}

  *  $\varepsilon_t=\frac{y_t-(\ell_{t-1}+b_{t-1})}{(\ell_{t-1}+b_{t-1})}$

  *  $\beta=\alpha \beta^*$ and $\varepsilon_t \sim \text{NID}(0,\sigma^2)$.

## Additive error models

\placefig{0}{1.6}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_add.pdf}

## Multiplicative error models

\placefig{0}{1.6}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_multi.pdf}

## Estimating ETS models

  * Smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$ are estimated by maximising the "likelihood" = the probability of the data arising from the specified model.
  * For models with additive errors equivalent to minimising SSE.
  * For models with multiplicative errors, \textbf{not} equivalent to minimising SSE.
  * We will estimate models with the \Verb|ets()| function in the forecast package.

## Model selection
\fontsize{13}{15}\sf

\begin{block}{Akaike's Information Criterion}
\[
\text{AIC} = -2\log(\text{L}) + 2k
\]
\end{block}\vspace*{-0.2cm}
where $L$ is the likelihood and $k$ is the number of parameters initial states estimated in the model.\pause

\begin{block}{Corrected AIC}
\[
\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k}
\]
\end{block}\vspace*{-0.3cm}
which is the AIC corrected (for small sample bias).
\pause
\begin{block}{Bayesian Information Criterion}
\[
\text{BIC} = \text{AIC} + k(\log(T)-2).
\]
\end{block}

## Automatic forecasting

**From Hyndman et al.\ (IJF, 2002):**

* Apply each model that is appropriate to the data.
Optimize parameters and initial values using MLE (or some other
criterion).
* Select best method using AICc.
* Produce forecasts using best method.
* Obtain prediction intervals using underlying state space model.

Method performed very well in M3 competition.

## Some unstable models

* Models with multiplicative errors are useful for strictly positive data, but are not numerically stable with data containing zeros or negative values. In that case only the six fully additive models will be applied.
* Some of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties.

## Exponential smoothing models
\fontsize{11}{12}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Additive Error}} &        \multicolumn{3}{c}{\bf Seasonal Component}         \\
          \multicolumn{2}{c|}{\bf Trend}         &         N         &         A         &         M         \\
        \multicolumn{2}{c|}{\bf Component}       &     ~(None)~      &    (Additive)     & (Multiplicative)  \\ \cline{3-5}
           &                                     &                   &                   &  \\[-0.3cm]
  N        & (None)                              &       A,N,N       &       A,N,A       &    \st{A,N,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A        & (Additive)                          &       A,A,N       &       A,A,A       &    \st{A,A,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                   &   A,A\damped,N    &   A,A\damped,A    & \st{A,A\damped,M}
\end{tabular}
\end{block}

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Multiplicative Error}} &     \multicolumn{3}{c}{\bf Seasonal Component}      \\
             \multicolumn{2}{c|}{\bf Trend}            &      N       &         A         &        M         \\
           \multicolumn{2}{c|}{\bf Component}          &   ~(None)~   &    (Additive)     & (Multiplicative) \\ \cline{3-5}
           &                                           &              &                   &  \\[-0.3cm]
  N        & (None)                                    &    M,N,N     &       M,N,A       &      M,N,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A        & (Additive)                                &    M,A,N     &       M,A,A       &      M,A,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                         & M,A\damped,N &   M,A\damped,A    &   M,A\damped,M
\end{tabular}
\end{block}

## Prediction intervals

\alert{Prediction intervals:} cannot be generated using the methods, only the models.

  * The prediction intervals will differ between models with additive and multiplicative errors.
  * Exact formulae for some models.
  * More general to simulate future sample paths, conditional on the last estimate of the states, and to obtain prediction intervals from the percentiles of these simulated future paths.

## Prediction intervals
\fontsize{12}{13}\sf\vspace*{-0.2cm}

PI for most ETS models: $\hat{y}_{T+h|T} \pm c \sigma_h$, where $c$ depends on coverage probability and $\sigma_h$ is forecast standard deviation.

\fontsize{10}{12}\sf\vspace*{0.2cm}

\hspace*{-0.8cm}\begin{tabular}{ll}
\hline
(A,N,N) & $\sigma_h = \sigma^2\big[1 + \alpha^2(h-1)\big]$\\
(A,A,N) & $\sigma_h = \sigma^2\Big[1 + (h-1)\big\{\alpha^2 + \alpha\beta h + \frac16\beta^2h(2h-1)\big\}\Big]$\\
(A,A$_d$,N) & $\sigma_h = \sigma^2\biggl[1 + \alpha^2(h-1) + \frac{\beta\phi h}{(1-\phi)^2} \left\{2\alpha(1-\phi) +\beta\phi\right\}$\\
      & \hspace*{1.5cm}$\mbox{} - \frac{\beta\phi(1-\phi^h)}{(1-\phi)^2(1-\phi^2)} \left\{ 2\alpha(1-\phi^2)+ \beta\phi(1+2\phi-\phi^h)\right\}\biggr]$\\
(A,N,A) &              $\sigma_h = \sigma^2\Big[1 + \alpha^2(h-1) + \gamma k(2\alpha+\gamma)\Big]$\\
(A,A,A) &              $\sigma_h = \sigma^2\Big[1 + (h-1)\big\{\alpha^2 + \alpha\beta h + \frac16\beta^2h(2h-1)\big\} + \gamma k \big\{2\alpha+ \gamma + \beta m (k+1)\big\} \Big]$\\
(A,A$_d$,A) &  $\sigma_h = \sigma^2\biggl[1 + \alpha^2(h-1) +\frac{\beta\phi h}{(1-\phi)^2} \left\{2\alpha(1-\phi)  + \beta\phi \right\}$\\
  & \hspace*{1.5cm}$\mbox{} - \frac{\beta\phi(1-\phi^h)}{(1-\phi)^2(1-\phi^2)} \left\{ 2\alpha(1-\phi^2)+ \beta\phi(1+2\phi-\phi^h)\right\}$ \\
  & \hspace*{1.5cm}$\mbox{} + \gamma k(2\alpha+\gamma)  + \frac{2\beta\gamma\phi}{(1-\phi)(1-\phi^m)}\left\{k(1-\phi^m) - \phi^m(1-\phi^{mk})\right\}\biggr]$
\end{tabular}

# ETS in R

## Example: drug sales
\fontsize{9}{8}\sf

```{r, echo=TRUE}
ets(h02)
```

## Example: drug sales
\fontsize{9}{8}\sf

```{r, echo=TRUE}
ets(h02, model="AAA", damped=FALSE)
```

## The `ets()` function

* Automatically chooses a model by default using the AIC, AICc or BIC.
* Can handle any combination of trend, seasonality and damping
* Produces prediction intervals for every model
* Ensures the parameters are admissible (equivalent to invertible)
* Produces an object of class "ets".

## `ets` objects

* **Methods:** `coef()`, `autoplot()`, `plot()`, `summary()`, `residuals()`, `fitted()`, `simulate()` and `forecast()`
* `autoplot()` and `plot()` functions show time plots of the original time series along with the extracted components (level, growth and seasonal).

## Example: drug sales
```{r, echo=TRUE, fig.height=4}
h02 %>% ets() %>% autoplot()
```

## Example: drug sales

```{r, echo=TRUE, fig.height=4}
h02 %>% ets() %>% forecast() %>% autoplot()
```

## Example: drug sales
\fontsize{11}{12}\sf

```{r, echo=TRUE}
h02 %>% ets() %>% accuracy()

h02 %>% ets(model="AAA", damped=FALSE) %>% accuracy()
```

## The `ets()` function
\fontsize{10}{9.5}\sf

`ets()` function also allows refitting model to new data set.

```{r, echo=TRUE}
train <- window(h02, end=c(2004,12))
test <- window(h02, start=2005)
fit1 <- ets(train)
fit2 <- ets(test, model = fit1)
accuracy(fit2)
accuracy(forecast(fit1,10), test)
```

## The `ets()` function in R
\fontsize{11.5}{14}\sf

```r
ets(y, model = "ZZZ", damped = NULL,
  additive.only = FALSE,
  lambda = NULL, biasadj = FALSE,
  lower = c(rep(1e-04, 3), 0.8),
  upper = c(rep(0.9999, 3), 0.98),
  opt.crit = c("lik", "amse", "mse", "sigma", "mae"),
  nmse = 3,
  bounds = c("both", "usual", "admissible"),
  ic = c("aicc", "aic", "bic"),
  restrict = TRUE,
  allow.multiplicative.trend = FALSE, ...)
```

## The `ets()` function in R
\fontsize{13}{14}\sf\vspace*{-0.2cm}

* `y` \newline The time series to be forecast.
* `model` \newline use the ETS classification and notation: "N" for none, "A" for additive, "M" for multiplicative, or "Z" for automatic selection. Default `ZZZ` all components are selected using the information criterion.
* `damped`
  * If `damped=TRUE`, then a damped trend will be used (either A\damped\ or M\damped).
  * `damped=FALSE`, then a non-damped trend will used.
  * If `damped=NULL` (default), then either a damped or a non-damped trend will be selected according to the information criterion chosen.

## The `ets()` function in R
\fontsize{12.5}{14}\sf\vspace*{-0.2cm}

  * `additive.only`\newline
      Only models with additive components will be considered if \Verb"additive.only=TRUE". Otherwise all models will be considered.
  * `lambda`\newline
      Box-Cox transformation parameter. Ignored if `lambda=NULL` (default). Otherwise, the time series will be transformed before the model is estimated. When `lambda` is not `NULL`, `additive.only` is set to `TRUE`.
  * `biadadj`\newline
      Uses bias-adjustment when undoing Box-Cox transformation for fitted values.
  * `allow.multiplicative.trend` allows models with a multiplicative trend.

## The `forecast()` function in R
\fontsize{12}{14}\sf

```r
  forecast(object,
    h=ifelse(object$m>1, 2*object$m, 10),
    level=c(80,95), fan=FALSE,
    simulate=FALSE, bootstrap=FALSE,
    npaths=5000, PI=TRUE,
    lambda=object$lambda, biasadj=FALSE,...)
```

* `object`: the object returned by the \Verb|ets()| function.
* `h`: the number of periods to be forecast.
* `level`: the confidence level for the prediction intervals.
* `fan`: if `fan=TRUE`, suitable for fan plots.
* `simulate`: If `TRUE`, prediction intervals generated via simulation rather than analytic formulae. Even if `FALSE` simulation will be used if no algebraic formulae exist.

## The `forecast()` function in R
\fontsize{13}{15}\sf

* `bootstrap`: If `bootstrap=TRUE` and \Verb"simulate=TRUE", then simulated prediction intervals use re-sampled errors rather than normally distributed errors.
* `npaths`: The number of sample paths used in computing simulated prediction intervals.
* `PI`: If `PI=TRUE`, then prediction intervals are produced; otherwise only point forecasts are calculated. If `PI=FALSE`, then `level`, `fan`, `simulate`, `bootstrap` and `npaths` are all ignored.
* `lambda`: The Box-Cox transformation parameter. Ignored if `lambda=NULL`. Otherwise, forecasts are back-transformed via inverse Box-Cox transformation.
* `biasadj`: Apply bias adjustment after Box-Cox?

# Lab session 9
##
\fontsize{48}{60}\sf\centering
**Lab Session 9**

# Lab session 10
##
\fontsize{48}{60}\sf\centering
**Lab Session 10**

